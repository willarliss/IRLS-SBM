{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_array\n",
    "from scipy.sparse import diags\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "probs = np.array([\n",
    "    [0.9, 0.01, 0.01],\n",
    "    [0.01, 0.9, 0.01],\n",
    "    [0.01, 0.01, 0.9],\n",
    "])\n",
    "# probs = np.array([\n",
    "#     [0.4, 0.01, 0.01],\n",
    "#     [0.01, 0.4, 0.01],\n",
    "#     [0.01, 0.01, 0.4],\n",
    "# ])\n",
    "# probs = np.array([\n",
    "#     [0.4, 0.4, 0.01],\n",
    "#     [0.4, 0., 0.01],\n",
    "#     [0.01, 0.01, 0.42],\n",
    "# ])\n",
    "# sizes = rng.poisson(111, size=3)\n",
    "sizes = rng.poisson(555, size=3)\n",
    "graph = nx.stochastic_block_model(sizes, probs, seed=rng)\n",
    "labels = [b for n, b in graph.nodes(data='block')]\n",
    "num = sizes.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 123\n",
    "# X += np.random.default_rng(SEED).normal(size=(len(G.nodes), k), scale=1/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "\n",
    "def hardmax(X):\n",
    "    Y = np.zeros_like(X)\n",
    "    Y[np.arange(X.shape[0]), X.argmax(1)] = 1\n",
    "    return Y\n",
    "\n",
    "def sbm_slow(G, k, *,\n",
    "             likelihood='bernoulli',\n",
    "             alpha=0.,\n",
    "             weight=None,\n",
    "             track_scores=False,\n",
    "             max_iter=100,\n",
    "             min_epochs=10,\n",
    "             tol=0.01):\n",
    "    # This approach implements the method exactly as derived\n",
    "\n",
    "    # Adjacency matrix\n",
    "    A = nx.to_scipy_sparse_array(G, weight=weight).astype(float)\n",
    "    A_dense = A.toarray() if track_scores else None\n",
    "\n",
    "    if likelihood == 'bernoulli':\n",
    "        assert ((A.data==0) | (A.data==1)).all()\n",
    "    elif likelihood == 'poisson':\n",
    "        assert (A.data >= 0).all() and (A.data == A.data.round()).all()\n",
    "    elif likelihood == 'normal':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # Soft partition matrix\n",
    "    X = np.ones((len(G.nodes), k)) / k\n",
    "    X += np.random.randn(len(G.nodes), k) / 100\n",
    "    # Hard partition matrix\n",
    "    Z = hardmax(X)\n",
    "    partition = Z.argmax(1)\n",
    "\n",
    "    # Structure matrix sufficient statistics\n",
    "    M = Z.T @ (A @ Z)\n",
    "    n = Z.sum(0)[:, None]\n",
    "    # Structure matrix MLE\n",
    "    B = M / (n@n.T).clip(1, None)\n",
    "\n",
    "    # Regularization\n",
    "    R = np.eye(k) * alpha\n",
    "\n",
    "    if track_scores:\n",
    "        # Initialize trace of scores\n",
    "        P = np.clip(Z@B@Z.T, eps, 1-eps)\n",
    "        if likelihood == 'bernoulli':\n",
    "            L = A_dense * np.log(P) + (1-A_dense) * np.log(1-P)\n",
    "        elif likelihood == 'poisson':\n",
    "            L = A_dense * np.log(P) - P\n",
    "        elif likelihood == 'normal':\n",
    "            L = 1/2 * (A_dense - P)**2\n",
    "        trace = [L.mean()]\n",
    "\n",
    "    for epoch in range(max_iter):\n",
    "\n",
    "        # Compute predictions\n",
    "        P = np.clip(Z@B@Z.T, eps, 1-eps)\n",
    "        if likelihood == 'bernoulli':\n",
    "            w = (1 / P / (1-P)).mean(1)\n",
    "        elif likelihood == 'poisson':\n",
    "            w = (1 / P).mean(1)\n",
    "        elif likelihood == 'normal':\n",
    "            w = np.ones(len(G.nodes))\n",
    "\n",
    "        # Perform fisher scoring updates\n",
    "        W = diags(w)\n",
    "        hess = B @ Z.T @ (W @ Z) @ B.T + R\n",
    "        grad = (A.T @ W @ Z @ B.T).T\n",
    "        X = (X - Z) + np.linalg.solve(hess, grad).T\n",
    "\n",
    "        # Recompute structure matrix\n",
    "        Z = hardmax(X)\n",
    "        M = Z.T @ (A @ Z)\n",
    "        n = Z.sum(0)[:, None]\n",
    "        B = M / (n@n.T).clip(1, None)\n",
    "\n",
    "        # Early stopping\n",
    "        prev_partition = partition\n",
    "        partition = Z.argmax(1)\n",
    "        if epoch > min_epochs and (prev_partition == partition).mean() > 1-tol:\n",
    "            print('converged in', epoch+1, 'iterations')\n",
    "            break\n",
    "\n",
    "        # Append current score to trace\n",
    "        if track_scores:\n",
    "            P = np.clip(Z@B@Z.T, eps, 1-eps)\n",
    "            if likelihood == 'bernoulli':\n",
    "                L = A_dense * np.log(P) + (1-A_dense) * np.log(1-P)\n",
    "            elif likelihood == 'poisson':\n",
    "                L = A_dense * np.log(P) - P\n",
    "            elif likelihood == 'normal':\n",
    "                L = 1/2 * (A_dense - P)**2\n",
    "            trace.append(L.mean())\n",
    "\n",
    "    else:\n",
    "        print('did not converge after', max_iter, 'iterations')\n",
    "\n",
    "    if track_scores:\n",
    "        return partition, np.array(trace)\n",
    "    return partition\n",
    "\n",
    "\n",
    "def sbm_fast(G, k, *,\n",
    "             likelihood='bernoulli',\n",
    "             alpha=0.,\n",
    "             weight=None,\n",
    "             track_scores=False,\n",
    "             max_iter=100,\n",
    "             min_epochs=10,\n",
    "             tol=0.01):\n",
    "    # This approach implements the method with more computational efficiency\n",
    "\n",
    "    # Adjacency matrix\n",
    "    A = nx.to_scipy_sparse_array(G, weight=weight).astype(float)\n",
    "    A_dense = A.toarray() if track_scores else None\n",
    "\n",
    "    if likelihood == 'bernoulli':\n",
    "        assert ((A.data==0) | (A.data==1)).all()\n",
    "    elif likelihood == 'poisson':\n",
    "        assert (A.data >= 0).all() and (A.data == A.data.round()).all()\n",
    "    elif likelihood == 'normal':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # Soft partition matrix\n",
    "    X = np.ones((len(G.nodes), k)) / k\n",
    "    X += np.random.randn(len(G.nodes), k) / 100\n",
    "    # Hard partition matrix\n",
    "    Z = hardmax(X)\n",
    "    partition = Z.argmax(1)\n",
    "\n",
    "    # Structure matrix sufficient statistics\n",
    "    M = Z.T @ (A @ Z)\n",
    "    n = Z.sum(0)[:, None]\n",
    "    # Structure matrix MLE\n",
    "    B = M / (n@n.T).clip(1, None)\n",
    "\n",
    "    # Regularization\n",
    "    R = np.eye(k) * alpha\n",
    "\n",
    "    if track_scores:\n",
    "        # Initialize trace of scores\n",
    "        P = np.clip(Z@B@Z.T, eps, 1-eps)\n",
    "        if likelihood == 'bernoulli':\n",
    "            L = A_dense * np.log(P) + (1-A_dense) * np.log(1-P)\n",
    "        elif likelihood == 'poisson':\n",
    "            L = A_dense * np.log(P) - P\n",
    "        elif likelihood == 'normal':\n",
    "            L = 1/2 * (A_dense - P)**2\n",
    "        trace = [L.mean()]\n",
    "\n",
    "    for epoch in range(max_iter):\n",
    "\n",
    "        # Compute predictions\n",
    "        if likelihood == 'bernoulli':\n",
    "            w_pre = 1 / (B * (1 - B)).clip(eps, None)\n",
    "        elif likelihood == 'poisson':\n",
    "            w_pre = 1 / B.clip(eps, None)\n",
    "        elif likelihood == 'normal':\n",
    "            w_pre = np.ones_like(B)\n",
    "        w_block = (w_pre * n.T).sum(axis=1) / n.sum()\n",
    "        w = w_block[partition]\n",
    "\n",
    "        # Perform fisher scoring updates\n",
    "        ZB = B.T[partition, :]\n",
    "        ZBW = ZB * w[:, None]\n",
    "        hess = ZB.T @ ZBW + R\n",
    "        grad = (A.T @ ZBW).T\n",
    "        X = (X - Z) + np.linalg.solve(hess, grad).T\n",
    "\n",
    "        # Recompute structure matrix\n",
    "        Z = hardmax(X)\n",
    "        M = Z.T @ (A @ Z)\n",
    "        n = Z.sum(0)[:, None]\n",
    "        B = M / (n@n.T).clip(1, None)\n",
    "\n",
    "        # Early stopping\n",
    "        prev_partition = partition\n",
    "        partition = Z.argmax(1)\n",
    "        if epoch > min_epochs and (prev_partition == partition).mean() > 1-tol:\n",
    "            print('converged in', epoch+1, 'iterations')\n",
    "            break\n",
    "\n",
    "        # Append current score to trace\n",
    "        if track_scores:\n",
    "            P = np.clip(Z@B@Z.T, eps, 1-eps)\n",
    "            if likelihood == 'bernoulli':\n",
    "                L = A_dense * np.log(P) + (1-A_dense) * np.log(1-P)\n",
    "            elif likelihood == 'poisson':\n",
    "                L = A_dense * np.log(P) - P\n",
    "            elif likelihood == 'normal':\n",
    "                L = 1/2 * (A_dense - P)**2\n",
    "            trace.append(L.mean())\n",
    "\n",
    "    else:\n",
    "        print('did not converge after', max_iter, 'iterations')\n",
    "\n",
    "    if track_scores:\n",
    "        return partition, np.array(trace)\n",
    "    return partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b63e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit sbm_slow(graph, 3, max_iter=100, tol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit sbm_fast(graph, 3, max_iter=100, tol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1, t1 = sbm_slow(graph, 3, max_iter=100, track_scores=True, tol=0.)\n",
    "# p2, t2 = sbm_fast(graph, 3, max_iter=100, track_scores=True, tol=0.)\n",
    "# np.abs(p1-p2).mean(), np.abs(t1-t2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efa06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sbm_fast(graph, 3, alpha=0.1, tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafa45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab796cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, scores = sbm_fast(graph, 3, alpha=0.01, tol=0.01, track_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a4768",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05c75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "334d48f4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(X, k, *,\n",
    "            alpha=0.,\n",
    "            max_iter=100,\n",
    "            min_epochs=10,\n",
    "            tol=0.01):\n",
    "\n",
    "    # Similarity matrix\n",
    "    X_norm = X / np.linalg.norm(X, 2, axis=1, keepdims=True)\n",
    "    A = X_norm @ X_norm.T\n",
    "\n",
    "    # Soft partition matrix\n",
    "    X = np.ones((X.shape[0], k)) / k\n",
    "    X += np.random.randn(X.shape[0], k) / 100\n",
    "    # Hard partition matrix\n",
    "    Z = hardmax(X)\n",
    "    partition = Z.argmax(1)\n",
    "\n",
    "    # Structure matrix sufficient statistics\n",
    "    M = Z.T @ A @ Z\n",
    "    n = Z.sum(0)[:, None]\n",
    "    # Structure matrix MLE\n",
    "    B = M / (n@n.T).clip(1, None)\n",
    "\n",
    "    # Regularization\n",
    "    R = np.eye(k) * alpha\n",
    "\n",
    "    for epoch in range(max_iter):\n",
    "\n",
    "        # Perform fisher scoring updates\n",
    "        ZB = B.T[partition, :]\n",
    "        hess = ZB.T @ ZB + R\n",
    "        grad = ZB.T @ A\n",
    "        X = (X - Z) + np.linalg.solve(hess, grad).T\n",
    "\n",
    "        # Recompute structure matrix\n",
    "        Z = hardmax(X)\n",
    "        M = Z.T @ A @ Z\n",
    "        n = Z.sum(0)[:, None]\n",
    "        B = M / (n@n.T).clip(1, None)\n",
    "\n",
    "        # Early stopping\n",
    "        prev_partition = partition\n",
    "        partition = Z.argmax(1)\n",
    "        if epoch > min_epochs and (prev_partition == partition).mean() > 1-tol:\n",
    "            print('converged in', epoch+1, 'iterations')\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        print('did not converge after', max_iter, 'iterations')\n",
    "\n",
    "    return partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=300, n_features=10,\n",
    "                           n_clusters_per_class=1, n_classes=3, flip_y=0, class_sep=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster(X, 3, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24baae9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "340d115e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hardmax(X):\n",
    "    Y = np.zeros_like(X)\n",
    "    col_idx = X.argmax(1)\n",
    "    row_idx = np.arange(X.shape[0])\n",
    "    mask = X[row_idx, col_idx] > 0\n",
    "    Y[row_idx[mask], col_idx[mask]] = 1\n",
    "    return Y\n",
    "\n",
    "def sbm_slow2(G, k, *,\n",
    "              likelihood='bernoulli',\n",
    "              alpha=0.,\n",
    "              beta=0.0001,\n",
    "              weight=None,\n",
    "              track_scores=False,\n",
    "              max_iter=100,\n",
    "              min_epochs=10,\n",
    "              tol=0.01):\n",
    "    # This approach implements the method exactly as derived\n",
    "\n",
    "    # Adjacency matrix\n",
    "    A = nx.to_scipy_sparse_array(G, weight=weight).astype(float)\n",
    "    A_dense = A.toarray() if track_scores else None\n",
    "\n",
    "    if likelihood == 'bernoulli':\n",
    "        assert ((A.data==0) | (A.data==1)).all()\n",
    "    elif likelihood == 'poisson':\n",
    "        assert (A.data >= 0).all() and (A.data == A.data.round()).all()\n",
    "    elif likelihood == 'normal':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # Soft partition matrix\n",
    "    X = np.ones((len(G.nodes), k)) / k\n",
    "    X += np.random.randn(len(G.nodes), k) / 100\n",
    "    # Hard partition matrix\n",
    "    Z = hardmax(X)\n",
    "    partition = Z.argmax(1)\n",
    "\n",
    "    # Structure matrix sufficient statistics\n",
    "    M = Z.T @ (A @ Z)\n",
    "    n = Z.sum(0)[:, None]\n",
    "    # Structure matrix MLE\n",
    "    B = M / (n@n.T).clip(1, None)\n",
    "\n",
    "    # Regularization\n",
    "    R = np.eye(k) * alpha\n",
    "\n",
    "    if track_scores:\n",
    "        # Initialize trace of scores\n",
    "        P = np.clip(Z@B@Z.T, eps, 1-eps)\n",
    "        if likelihood == 'bernoulli':\n",
    "            L = A_dense * np.log(P) + (1-A_dense) * np.log(1-P)\n",
    "        elif likelihood == 'poisson':\n",
    "            L = A_dense * np.log(P) - P\n",
    "        elif likelihood == 'normal':\n",
    "            L = 1/2 * (A_dense - P)**2\n",
    "        trace = [L.mean()]\n",
    "\n",
    "    for epoch in range(max_iter):\n",
    "\n",
    "        # Compute predictions\n",
    "        P = np.clip(Z@B@Z.T, eps, 1-eps)\n",
    "        if likelihood == 'bernoulli':\n",
    "            w = (1 / P / (1-P)).mean(1)\n",
    "        elif likelihood == 'poisson':\n",
    "            w = (1 / P).mean(1)\n",
    "        elif likelihood == 'normal':\n",
    "            w = np.ones(len(G.nodes))\n",
    "\n",
    "        # Perform fisher scoring updates\n",
    "        W = diags(w)\n",
    "        hess = B @ Z.T @ (W @ Z) @ B.T + R\n",
    "        grad = (A.T @ W @ Z @ B.T).T\n",
    "        # grad -= beta * np.sign(X.sum(0)).reshape(-1, 1)\n",
    "        if epoch > min_epochs**0.5:\n",
    "            grad -= beta * np.sign(X.sum(0)).reshape(-1, 1)\n",
    "        X = (X - Z) + np.linalg.solve(hess, grad).T\n",
    "\n",
    "        # Recompute structure matrix\n",
    "        Z = hardmax(X)\n",
    "        M = Z.T @ (A @ Z)\n",
    "        n = Z.sum(0)[:, None]\n",
    "        B = M / (n@n.T).clip(1, None)\n",
    "\n",
    "        # Early stopping\n",
    "        prev_partition = partition\n",
    "        partition = Z.argmax(1)\n",
    "        if epoch > min_epochs and (prev_partition == partition).mean() > 1-tol:\n",
    "            print('converged in', epoch+1, 'iterations')\n",
    "            break\n",
    "\n",
    "        # Append current score to trace\n",
    "        if track_scores:\n",
    "            P = np.clip(Z@B@Z.T, eps, 1-eps)\n",
    "            if likelihood == 'bernoulli':\n",
    "                L = A_dense * np.log(P) + (1-A_dense) * np.log(1-P)\n",
    "            elif likelihood == 'poisson':\n",
    "                L = A_dense * np.log(P) - P\n",
    "            elif likelihood == 'normal':\n",
    "                L = 1/2 * (A_dense - P)**2\n",
    "            trace.append(L.mean())\n",
    "\n",
    "    else:\n",
    "        print('did not converge after', max_iter, 'iterations')\n",
    "\n",
    "    if track_scores:\n",
    "        return partition, np.array(trace)\n",
    "    return partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sbm_slow2(graph, 7, alpha=0.01, beta=10, tol=0.01, track_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85313d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6aaf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ffa236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aca54999",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0509d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df706533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b5d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
